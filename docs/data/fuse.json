{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"# Welcome to Dendron\n\nThis is the root of your dendron vault. If you decide to publish your entire vault, this will be your landing page. You are free to customize any part of this page except the frontmatter on top.\n\n## Lookup\n\nThis section contains useful links to related resources.\n\n- [Getting Started Guide](https://link.dendron.so/6b25)\n- [Discord](https://link.dendron.so/6b23)\n- [Home Page](https://wiki.dendron.so/)\n- [Github](https://link.dendron.so/6b24)\n- [Developer Docs](https://docs.dendron.so/)","n":0.132}}},{"i":2,"$":{"0":{"v":"School","n":1}}},{"i":3,"$":{"0":{"v":"KBAI","n":1},"1":{"v":"\n[Homepage](https://lucylabs.gatech.edu/kbai/summer-2023/)\n\n[Lectures](https://omscs.gatech.edu/cs-7637-knowledge-based-artificial-intelligence-course-videos)\n\n![Cognative Systems](./assets/cognitive_systems.png)\n\n## Intro to KBAI\n\nCharacteristics of AI Problems\n\n- Knowledge arrives incrementally\n- Problems exhibit recurring patterns\n- Problems have multiple levels of granularity\n- Problems are computationall intractable\n- The world is dynamic, but knowledge is static\n- The world is open-ended, but knowledge is finite\n\nCharacteristics of AI Agents\n\n- Limited computing power\n- Limited sensors\n- Limited attention\n- Computational logic is fundamentally deductive\n- Agent knowledge is incomplete relative to the world\n\n### Knowledge Representations\n\n- [Semantic Networks](./School.KBAI.Fundamentals.Semantic_Networks.md)\n\n### Problem Solving Techniques\n\n- [Generate & Test](./School.KBAI.Fundamentals.Generate_and_Test.md)\n\n### Architectures\n\n- []()\n\n### Fundamental Processes of KBAI Cognative Systems\n\n- \"Cognative\" means dealing with human-like intelligence\n- *Cognative system and KBAI-agent are used interchangably*\n- Deliberation, Meta-Cognition and Reaction make the three-layer system\n\n#### Deliberation\n\nReasoning about the world around us.\n\n- Reasoning\n- Learning\n- Memory: Knowledge gained from learning needs to be stored\n\nthink of being in traffic and wanting to change lanes. Requires planning\n\n#### Meta-cognition\n\nReasoning about deliberation or reaction.\n\nLane-change results in another car honking at you. You need to reason about the cause of the sub-optimal result.\n\n#### Reaction\n\n- A direct mapping from perception to action\n\n### Kinds of KBAI\n\n- Think/Act\n- Optimally/Like Humans\n\n- Think like humans: Semantic Web\n- Think optimally: Machine Learning\n- Act like humans: Improvisational Robotics\n- Act optimally: Airplane Auto-pilot\n\n### Principles of KBAI\n\n- KBAI agents represent and organize knowledge into knowledge structures to guide and support reasoning\n- Learning in KBAI agents is often incremental\n- Reasoning in KBAI agents is top-down as well as bottom-up\n- KBAI agents match methods to tasks\n- KBAI agents use heuristics to find solutions that are good enough, though not necessarily optimal\n- KBAI agenbts make use of recurring patterns in the problems they solve\n- The architecture of KBAI agents enables reasoning, learning and memory to support and constrain each other\n","n":0.061}}},{"i":4,"$":{"0":{"v":"Visuospatial_Reasoning","n":1}}},{"i":5,"$":{"0":{"v":"Visuospatial_Reasoning","n":1}}},{"i":6,"$":{"0":{"v":"Constraint_Propagatio","n":1},"1":{"v":"\nConstraint Propagation: A method of inference that assigns values to variables characterizing a problemin such a way that some conditions (constraints) are satisfied.\n\n## Image Processing\n\n## Natural Language Understanding\n\n## Advanced Problems\n","n":0.183}}},{"i":7,"$":{"0":{"v":"Planning","n":1}}},{"i":8,"$":{"0":{"v":"Planning","n":1}}},{"i":9,"$":{"0":{"v":"Logic","n":1},"1":{"v":"\n$$\n\\vee, \\lor\n\\wedge\n\\colon\n\\oplus\n\\otimes\n\\lnot\n\\forall\n\\exists\n$$\n\n## Formal Notation\n\n- Inference: agent will use knowledge and logic to provide inference\n- Soundness: Only valid conclusions can be proven\n- Completeness: All valid conclusions can be proven\n\n- Predicate: a function that maps object args to T/F\n  - Feathers(bluebird) >>> True\n\nImplication x implies y, $\\Rightarrow$\n\n## Conjunctions, Disjunctions, Negations, Implications\n\n### Conjunction\n\nif an animal lays eggs and an animal flies than the animal is a bird\n\n$$\n\\text{If Lays-eggs(animal)} \\wedge \\text{Flies(animal)} \\Rightarrow \\text{Then Bird(animal)}\n$$\n\n### Disjunction\n\nif an animal lays eggs or an animal flies than the animal is a bird\n$$\n\\text{Lays-eggs(animal)} \\vee \\text{Flies(animal)} \\Rightarrow \\text{Then Bird(animal)}\n$$\n\nIf an animal flies and is not a bird, it is a bat\n$$\n\\text{Flies(animal)} \\wedge \\lnot\\text{Bird(animal)} \\Rightarrow \\text{Bat(animal)}\n$$\n\n\n## Truth Tables\n\n- Demorgan's law $\\lnot(A\\wedge B) == \\lnot A \\vee \\lnot B$\n  - The outer not flips the inner operation\n\nKinda weird logic for \"Implies\" $\\Rightarrow$\n\n|A|B|$A\\Rightarrow B$|\n|--|--|:-:|\n|T|T|T|\n|T|F|F|\n|F|T|T|\n|F|F|T|\n\n$A \\Rightarrow B == \\lnot A \\vee B$\n\n## Rules of Inference\n\n### Modus Ponens\n\n|x|Logic|\n|--|--|\n|Sentence 1| $P \\Rightarrow Q$ |\n|Sentence 2| $P$ |\n|Sentence 3| $Q$ |\n\n### Modus Tollens\n\n|x|Logic|\n|--|--|\n|Sentence 1| $P \\Rightarrow Q$ |\n|Sentence 2| $\\lnot Q$ |\n|Sentence 3| $\\lnot P$ |\n\n## Resolution Theorem Proving\n\n- Prove the opposite of what we're trying to prove\n- Start by eliminating the thing you're trying to prove with the contrapositive in the full sentence\n","n":0.071}}},{"i":10,"$":{"0":{"v":"Metacognition","n":1}}},{"i":11,"$":{"0":{"v":"Meta-Reasoning","n":1},"1":{"v":"\n## Mistakes in Knowledge, Reasoning and Learning\n\n## Gaps in Knowledge and Reasoning\n\nForm reasoning goals to ask itself to connect gaps in knowledge using other cases from memory\n\n## Streategy Selection and Integration\n\nstrategy integration is shifting from one strategy to another.\n\nEx. Metacognition chooses generate-and-test for all the strategies and determines the best, then moves to means-end analysis because it's best.\n\n## Meta-cognition\n\n![cognitive systems](./assets/cognitive_systems.png)\n\nMetacognition and deliberation are overlapping\n\nMetacognition helps w/ strategy selection\n\n- constraint propagation\n- case-based reasoning\n- planning\n- etc.\n\nFor each strategy it can select sub-tasks, recursively.\n\nRepresented by a two layer loop between deliberation and meta-cognition\n\n## Goal Based Autonomy\n","n":0.104}}},{"i":12,"$":{"0":{"v":"Learning_by_Correcting_Mistakes","n":1},"1":{"v":"\n## Explanation-based Learning Revisited\n\nExplain why an error led to a failure\n\n- The cup example of explaining what a cup is based on attribtutes.\n\n![cup explanation](./assets/cup_explanation.png)\n\nThe agent created an explanation of a cup, but it was incorrect, how does it correct for what made the explanation incorrect?\n\n## Isolating Mistakes\n\nHow can the agent isolate the error in it's former model?\n\n- Venn diagram where left circle is characteristics of a positive example\n  - Right circle is characteristics of a negative example\n  - The right outer join of the venn diagram are \"false suspicious features\"\n\nIn order to attempt to correct, the agent can retry with just false suspicious features to see if that feature is a negative characteristic\n\nAlgo for isolating mistakes:\n\n```\nTo find all suspicious true-success relations:\n- Intersect all true success $\\bigcap T$\n- Union all false successes $\\bigcup T$\n- Remove assersions in union from intersection $\\bigcap T - \\bigcup F$\n\nTo find all suspicious false-success relations:\n- Intersect all false success $\\bigcap F$\n- Union all true success $\\bigcup T$\n- Remove all assersions in union from intersection $\\bigcap F - \\bigcup T$\n```\n\n## Explaining Mistakes\n\nHow can the agent explain the problem that led to the error?\n\n\"Why is the handle being fixed important to an item being a cup?\"\n\nWithout explaining the mistakes, the agent can't truly learn and create optimal entries in the tree\n\n## Correcting Mistakes\n\nHow can the agent repair the model to prevent the error from recurring?\n\n- Errors can be in agent model or agent architecture\n","n":0.065}}},{"i":13,"$":{"0":{"v":"Ethics_in_Artifical_Intelligence","n":1},"1":{"v":"\nAre we doing the right things as computer scientists?\n\n- Defense applications?\n- What are the implications of robot soldiers\n- Should robot soldiers have morality?\n","n":0.209}}},{"i":14,"$":{"0":{"v":"Learning","n":1}}},{"i":15,"$":{"0":{"v":"Version_Spaces","n":1},"1":{"v":"\n## Algorithm for Version spaces\n\n1) Begin with two models, the most specialized and the most general model possible.\n2) If the example is a positive example, generalize the specalized model to include the positive example and prune away from the general model that cannot include it\n3) If the example is a negative example, specialize the general model to exclude the negative example, prune away from the specific models that do not exclude it\n4) Prune away any version spaces subsumed by earlier version spaces\n\n## Tree-Based Representations\n\n- Decision trees create more optimal trees, but require all the examples up-front\n- Discrimination trees create less optimal trees, but can learn incrementally\n","n":0.097}}},{"i":16,"$":{"0":{"v":"Learning_by_Recording_Cases","n":1},"1":{"v":"\n- Store cases that have been encountered to use in the future\n  - Similar to dynamic programming\n- Use nearest neighbor to find the most similar past case\n","n":0.192}}},{"i":17,"$":{"0":{"v":"Incremental_Concept_Learning","n":1},"1":{"v":"\n- Makes strong use of background knowledge\n\n1) Learning is incremental\n2) Examples are labeled (supervised)\n3) Examples can come in a particular order, positive then negative maybe\n4) Different from case-based reasoning as it uses abstract concepts\n5) Number of examples to abstract concepts from is small\n6) Generalize / Specialize trade-off\n\n![incremental concept learning](./assets/incremental_concept_learning.png)\n\n## Variabilization\n\n![variabilization](./assets/variabilization.png)\n\nTake a concrete concept like bricks stacked like a house and create abstract variables\nlike Brick and relationships like supports and left-of\n\n## Specialization\n\n- **Require link heuristic**, where some links are necessary to represent things not represented by a negative supervised example\n- **Forbid Link Heuristic**, opposite of require link, shows as logical not in link name\n\n## Generalization\n\n- **Drop link heuristic**, where we want to drop a link to turn somethin from too specific to more general is shown in [Variabilization](#variabilization)\n- **Enlarge-Set Heuristic**, where a node is combined to be several possible types\n- **Climb-Tree Heuristic**, where a type has inheritance, a block is an example of a brick or a wedge. IMPORTANT THIS IS DIRECTED AND **NOT BI-DIRECTIONAL**\n- **Close-Interval**, expand range of values to be a positive example of the concept\n\n## Heuristics for Specialization and Generalization\n\n\n","n":0.074}}},{"i":18,"$":{"0":{"v":"Classification","n":1},"1":{"v":"\n## Concept Learning\n\n## Equivalent Classes\n\n- Index actions based on equivalence class, which severely limits the usable input percepts\n- Break up a large problem into a large number of smaller problems\n\n## Concept Hierarchy\n\nVertibrate  -> Mammal\n            -> Bird     -> Eagle, Bluebird, Penguin\n            -> Reptile\n\n## Axiomatic Concept\n\n- Most formal\n- Easiest to talk about and teach a computer\n- Concept defined by a formal set of necessary and sufficient conditions\n  - definition of a circle, for example\n\n## Prototypical Concept\n\n- Mid formal\n- Base concept defined by a typical example with overridable properties\n- Well represented by frames\n\n## Exemplar Concept\n\n- Least formal\n- Concepts defined by implicit abstractions of instances, or exemplars of the concept\n  - \"This painting is beautiful\", what is beauty\n\n## Bottom-Up Search\n\n","n":0.093}}},{"i":19,"$":{"0":{"v":"Fundamentals","n":1}}},{"i":20,"$":{"0":{"v":"Semantic_Networks","n":1},"1":{"v":"\n\n## What is a Knowledge Representation?\n\n- Language, which has a vocabulary\n- Content, which goes into a representation, expressed in aforementioned language\n\n1) We identify each object, X, Y and Z\n![Semantic Representation](./assets/semantic_representation.png)\n\n2) Represent the relationship between the objects\n![Semantic Relationship](./assets/semantic_network_relationships.png)\n\nThe language of this semantic network are:\n\n- X, Y, Z\n- above, inside\n- expanded, unchanged, deleted, (contracted?)\n\n## Structure of Semantic Networks\n\n- Lexicon\n- Structure\n- Semantics\n\nRelating to the above example:\n\n- Lexicon: Nodes (X, Y, Z)\n- Structure: Directional Links (digraph/dag?)\n- Semantics: Application-specific labels\n\n## Represent and Reason\n\n- Represent different shapes\n- Reason about what transformations can be done to the shapes\n\n### Weights w/ Represent and Reason\n\nThere can be multiple partially of fully correct answers, we can assign weights to each transformation to be able to score which would be more likely to be chosen.\n","n":0.09}}},{"i":21,"$":{"0":{"v":"Production_Systems","n":1},"1":{"v":"\n## Cognative Architecture\n\n- A cognative agent is a function `f` that maps a perceptual history `P*` to an action `A`.\n- Percepts -> Actions\n- The `*` stands for \"history\"\n\n$$\nf: P^* \\rightarrow A\n$$\n\nAssumptions of Cognative Architectures:\n\n- Goal-oriented\n- Rich, complex environments\n- Significant knowledge\n- Represented at a level of abstraction, symbols\n- Respond flexibly to a changing environment\n- Learn from their experiences\n\n$$\n\\text{Architecture} + \\text{Content} = \\text{Behavior}\n$$\n\nIf architecture is fixed, then we only need to change the knowedge content of the agent to achieve different behaviors.\n\n### SOAR\n\n![SOAR](./assets/SOAR.png)\n\nConsists of a **long term memory** portion\n\n- Procedural knowledge (how to pour water into a container)\n- Semantic knowledge (concepts and models of the environment, model of how a plane flies in the air)\n- Episodic knowledge (events, think what was for dinner yesterday)\n\nand **short term memory**\n\n- Working memory\n\n## Production Systems\n\nLevels of Abstraction:\n\n- **High** - Task / Knowledge Level\n- **Mid** - Algorithm / Symbol Level\n- **Low** - Hardware / Implementation Level\n\n- Low level provides architecture for higher levels up ladder\n- High level provides content for lower levels down ladder\n\nWorking Memory Cycle\n\n- Working memory is the state of the system\n- Long term memory can consist of \"production rules\"\n- Based on long-term memory some rules get activated\n- Rules which are activated consequences get established\n- Consequences that are established mutate working memory\n\nIf SOAR reaches a state that it cannot determine an action for, it will envoke episodic knowledge to try and learn a new behavior\n\n## Action Selection\n\nMapping percepts in the world into actions\n\n- Percepts from the world into a pitch selection\n\n## Chunking\n\n- A **learning technique** that SOAR uses to overcome impasses\n- Uses episodic memory to find an event in the past that is related to current percepts and leverages the previous event into a new rule\n\n## Misc\n\nReasoning first, then work backwards to learning\n","n":0.059}}},{"i":22,"$":{"0":{"v":"Problem_Reduction","n":1}}},{"i":23,"$":{"0":{"v":"Means-Ends_Analysis","n":1},"1":{"v":"\n- If problems are well formed, means-ends analysis and problem reduction can work well\n- Weak method (makes little use of knowledge)\n\n## State Spaces\n\n- Initial state and goal state, the path along is a means of solving the problem\n\n## Means-Ends Analysis\n\n- Positive steps reduce the difference between the current state and the goal state, think the `a*` algorithm\n- Example of a universal AI method\n- No guarantee of optimality, or even being able to solve the problem\n\n## Problem Reduction\n\n- Decompose a hard problem into several smaller, easier problems\n- Think about how recurrsive solutions work to solve problems\n\n- In the block stacking example, decompose the final state into a series of \"milestones\" that are more likely to lead to a successful solution.\n- Does not provide guarantees of success\n","n":0.089}}},{"i":24,"$":{"0":{"v":"Generate_and_Test","n":1},"1":{"v":"\n- Given a problem, generate potential solutions and test the generated solutions\n\nThe loop is:\n\n1) Generate solutions\n2) Test generated solutions\n3) Prune/merge results to unque set w/ linking to parent/child states\n4) Repeat\n\n## Smart Generators\n\n- Creates possible next states\n- To make the generator \"smart\" it should not regenrate any duplicate or previous states, or generate illegal states\n\n## Smart Testers\n\n- Tester enforces problem constraints\n- \"Smart\" testers remove illegal states that have been generated\n\n## Generate and Test in an Unconstrained Domain\n","n":0.115}}},{"i":25,"$":{"0":{"v":"Evaluation","n":1}}},{"i":26,"$":{"0":{"v":"Ravens_Progressive_Matrices","n":1},"1":{"v":"\nAgent needs to understand the relationship of objects, how to transpose them, and the relation to some given option\n\nYou generate a hypothesis, and then you test it against the options\n\n- [Generate and Test](./School.KBAI.Fundamentals.Generate_and_Test.md)\n","n":0.174}}},{"i":27,"$":{"0":{"v":"Design_and_Creativity","n":1}}},{"i":28,"$":{"0":{"v":"Diagnosis","n":1},"1":{"v":"\nPrinciple of parsimony: Simplest possible explanation\n\n## Defining Doagnosis\n\nDiagnosis: To determins what is wrong with a malfunctioning device\nor the mapping of the data space to the hypothesis space\n\n- Malfunction is a discrepancy between expected results and observed results\n\n## Data and Hypothesis Spaces\n\n![data spaces](./assets/data_spaces.png)\n\n- The data space is all of the observed data\n- The hypothesis space is a hypothesis to explain the observed data\n- Mapping the data space to the observed space can be very difficult\n\n### Heuristic Classification\n\n- Bottom up classification processing follows the abstract arrow\n- top down classification processing follows refine arrow\n\n## Mapping Data to Hypothesis\n\nMapping needs to take place from data-space to hypothesis space, and additionally from the hypothesis space to the data space\n\n### Difficulties\n\n1) One data point, multiple hypothesis\n2) One hypothesis may explain multiple data points\n3) Multiple hypothesis multiple sets of data\n4) Mutually exclusive hypothesis\n5) Interacting data points (data points cancelling each other out)\n\n## Two Views of Diagnosis\n\n### Classification\n\nMapping data onto hypothsis\n\n### Abduction\n\nRule: Cause -> Effect\n\nRule: Bob hates Joe\nCause: Bob walks in\nEffect: Joe leaves\n\n### Fundamental Forms of Inference\n\n- Deduction: Given the rule and the cause, deduce the effect\n  - Modus pollen\n- Induction: Given a cause and effect, induce the rule\n- Given a rule and effect, abduce a cause\n\n### Choosing a Hypothesis\n\n1) Hypothesis must cover as much of the data as possible\n2) The smallest number of hypothesis out to be used, Principle of parsimony\n3) Some hypothesis may be more likely than others\n\n","n":0.066}}},{"i":29,"$":{"0":{"v":"Design","n":1},"1":{"v":"Design where not all parts are known (compositional design)\n\nidea - creative design program\n\n","n":0.277}}},{"i":30,"$":{"0":{"v":"Creativity","n":1},"1":{"v":"\nComputational creativity:\n\n- Novel - newness\n- Valuable\n- Unexpected - non-obvious\n- Emergence  - triangle emerges out of three random, intersecting lines\n- Re-representation - atomic structure / solar system\n- Serendipity\n- Conceptual Combination\n","n":0.186}}},{"i":31,"$":{"0":{"v":"Configuration","n":1},"1":{"v":"\n## Design & Configuration\n\n- We already know all the components, just need to configure to fit the problem\n\n- Input: needs, goals, functions\n- Output: Artifact that satisfies those needs, goals or functions\n- With design, both the problem and solution evolve together\n- Think AI agents that design AI agents\n\nConfiguration: A problem-solving activity that assigns values to variables to satisfy constraints.\n\n### Configureation Process\n\n![configuration process](./assets/configuration_process.png)\n\n- Start at the top left w/ specifications (think footprint of the basement example)\n- Begin abstract and refine/expand on each iteration\n- Middle method of abstract and partial solutions can be represented as a design plan\n  - Specifies all the variables of the plan\n  - ex. number of stories the house has\n- Lower level plan or refinement\n  - ex. plan for each of the stories of the house\n- the arrangement may cause new specifications to come about, the process repeats\n\n- Output the arrangement model\n\n## Plan Refinement\n\n## Connection to Earlier Topics\n\n### Classification\n\n- Similar to classification system of `loop(assign, refine)`\n- classification is a way of making sense of the world\n- configuration acts on the world\n\n### Case-Based Reasoning\n\n- CBR: Starting from a specific chair and tweaking as needed\n- Conf: starting w/ prototype concept and assign values and variables to meet constraints\n\n### Planning\n\n- A planner that generates a plan\n- Confiuguration\n","n":0.07}}},{"i":32,"$":{"0":{"v":"Common_Sense_Reasoning","n":1}}},{"i":33,"$":{"0":{"v":"Understanding","n":1},"1":{"v":"\n- Use background knowledge to disambiguate between similar sounding statements with different meanings\n\n## Thematic Role Systems\n\n### Analysis\n\n\"Ashok made pancakes for David with a griddle.\"\n\n#### Lexical analysis\n\n- categorize words, noun, verb, etc\n  - \"Ashok (noun) made (verb) pancakes for David with a griddle.\"\n- noun phrase and verb phrase encompassing multiple words\n  - \"Ashok(noun phrase) | made pancakes for David with a griddle(verb phrase)\"\n\n#### Semantic analysis\n\n- structure of the sentence like action, object, beneficiary and instrument\n  - \"Ashok(agent) made(verb) pancakes(thematic object) for David(beneficiary) with a griddle(instrument).\"\n- Frame representation\n\n```text\nThematic Role:\n- Verb: make\n- Agent: Ashok\n- Beneficiary: David\n- Thematic Object: Pancakes\n- Instrument: Griddle\n```\n\n- capture verbs\n- Single verbs can be ambiguous\n\n## Leveraging Constraints\n\n### Constraints\n\nConstraints imposed by the language help guide meaning\n\n| Preposition | Thematic Roles            |\n|-------------|---------------------------|\n|by           |Agent, conveyance, location|\n|for          |Beneficiary, duration      |\n|from         |Source                     |\n|to           |Destination                |\n|with         |Coagent, instrument        |\n\nExamples of by:\n\n- \"That was written by Ashok\" - Agent\n- \"David went to NY by train\" - Conveyance\n- \"David stood by the statue\" - Location\n\nIndividual verbs can be ambiguous\n\n!!! 14.09 resolving ambiguity in verbs\n","n":0.078}}},{"i":34,"$":{"0":{"v":"Scripts","n":1},"1":{"v":"\n## Definition\n\n## Form vs Content\n\n## Generating Expectations\n\n## Hierarchies of Scripts\n","n":0.316}}},{"i":35,"$":{"0":{"v":"Frames","n":1},"1":{"v":"\nA structured knowledge representation, \"the meaning of the meaning\"\n\nThink that a frame is a molecule rather than an atom\n\nCognative processing w/ frames is bottom up (from natural languang to representation)\nand top down (from memory to structured representation).\n\nStereotypes are cognatively efficient, so frames represent stereotypes\n\n## Structure of Frames\n\nA frame is a knowledge structure\n\n```\nAshok ate a frog:\n\nAte (verb frame)\n    Subject: Ashok\n    Object: Frog\n    Location: Stomach (now)\n    Time: past\n    Utensils: ?\n    Object-Alive: False\n    Object-is: in-subject\n    Subject-mood: Happy\n\n```\n\nSlots (Subject, Object, etc.) & Fillers (Ashok, Frog, etc.)\n\nSlots on a frame can have a pointer to another frame, for instance the `Subject`\nslot above could point to a noun-frame of Ashok. Allows for a discourse level\nunderstanding rather than a sentence level understanding.\n\n```\nAte (verb)\n    Subject: -> Ashok\n\nAshok (noun)\n    Title: Professor\n    Location: Atlanta, GA\n```\n\n## Properties of Frames\n\n- Frames represent stereotypes of the key word for the frame, like `Ate`\n- Provides default values\n- Takes advantage of inheritance and instances\n\n```\nAnimal\n    num-arms: (default value)\n    num-legs: (default value)\n\nHuman (type of animal) - Class\n    num-arms: 2\n    num-legs: 2\n    job:\n    name:\n\nHuman (type of animal) - Instance\n    num-arms: 2\n    num-legs: 2\n    job: Professor\n    name: Ashok\n```\n\n- Frames can be used to as a structured representation to store a sentence of input, and can be used to generate a sentence of output.\n\n## Representations\n\nSimilar to semantic nets, production systems\n\n![Procedural frame](./assets/Procedural_frame.png)\n\n1) Ate gets added to the working memory\n2) The slots and default filler values get loaded from semantic memory into working mem\n\n## Story Understanding\n","n":0.065}}},{"i":36,"$":{"0":{"v":"Common_Sense_Reasoning","n":1},"1":{"v":"\n## Primitive Actions\n\n- Move-body-part\n- Move-object\n- Expel\n- Ingest\n- Propel\n- Speak\n- See\n- Hear\n- Smell\n- Feel\n- Move-possession\n- Move-concept (idea to go somewhere for instance)\n- Think-about\n- Conclude\n\nPrimitives create an ontology of the world\n\nVerbs can be replaced with primitives:\n\n- shot -> propelled a bullet\n- fertalized -> spread fertilizer\n- decided -> conclude\n\nEach primitive should have an associated frame:\n\n```text\nAction Frame\n- primitive: propel\n- agent:\n- object:\n```\n\n```text\nAction Frame\n- primitive: move-possession\n- agent: [animate object before verb]\n- coagent:\n- object: [inanimate object after verb ]\n```\n\n## Hierarchical actions and sub-actions\n\n![action frame sub action](./assets/action_frame_sub_action.png)\n\n![multi verb](./assets/multi_verb.png)\n\n### Example\n\n\"Anika decided to have a glass of water\"\n\n1) decided (verb) -> conclude\n2) Conclude frame:\n\n```text\nAction\nprimitive: conclude\nagent: Anika\nresult:  ->\n```\n\n3) have -> ingest\n4) Ingest subframe:\n\n```text\nAction\nprimitive: ingest\nagent: Anika\nobject: glass of water\n```\n\n## State changes\n","n":0.096}}},{"i":37,"$":{"0":{"v":"Analogical_Reasoning","n":1}}},{"i":38,"$":{"0":{"v":"Learning_by_Recording_Cases","n":1}}},{"i":39,"$":{"0":{"v":"Explanation_Based_Learning","n":1},"1":{"v":"\nThe problem being discussed is what object could substitute for a cup if  no cups are available...\n\n## Concept Space\n\nHow does an agent determine if an object can work as a cup:\n\nPrior knowledge of cups:\n\n- Object -> has -> Bottom\n  - Bottom -> is -> Flat\n- Object -> made-of -> Porcelain\n- Object -> has -> Decoration\n- Object -> has -> Concavity\n- Object -> is -> Light\n- Object -> has -> Handle\n\nAgent wants to prove:\n\n- Object -> is -> Cup\n\nA prior knowledge relationship looks like this:\n\n![prior knowledge](./assets/briefcase_prior_knowledge.png)\n\nWhere the yellow arrows indicate causality\n\n## Abstraction\n\nAI agent has to abstract from other concepts to a concept that it can use for any object.\n\n1) A cup must be stable\n2) A brick is stable because it has a flat bottom\n3) A cup is stable because it has a flat bottom, like a brick\n\n![transfer learning](./assets/transfer_learning.png)\n\n## Analogical Transfer\n\n","n":0.085}}},{"i":40,"$":{"0":{"v":"Case_Based_Reasoning","n":1},"1":{"v":"\nProcess of case-based reasoning:\n\n1) Retrieval: think knn for retrieving a recorded case\n2) Adaptation: Adapt the prior solution to fit the current problem\n3) Evaluation: How well will the adapted solution address the current problem\n4) Storage: Store new case back into case-based reasoning repository\n\nAssumptions:\n\n- Patterns exist in the world\n- Similar problems have similar solutions\n\nFailed cases are useful to store as well\n\n## Adaptation\n\nproblem: Get from point A to point C\n\n### Model Based Method\n\n1) Retrieval finds a route from A to B\n2) Adaptation continues from B to C using a model of the streets\n3) Evaluation shows that A to C is satistied\n\n### Recursive retrieval\n\n1) No route from A to C\n2) Retrieve route from A to B (partial solution)\n3) Make new problem: Find route from B to C\n4) Retrieve route from B to C\n\n### Heuristics / Rules\n\nproblem: Get from point C to point A\n\n1) Retrieve route from A to C\n2) Reverse route to C to A - This may not work (think 1-way streets)\n\n## Storage\n\n### Indexing\n\n- route beginning x, y coordinates can be index key\n- Additionally, things like speed, scenic, etc can be added to make a richer index\n\n### Discrimination Tree\n\nA binary search tree of yes/no answers. Every time that a single node has multiple\nbranches pointing to it a new y/n question is created to split them.\n\nDoesn't have to be a binary tree, however each leaf has to have only one branch to it.\n\n![discrimination tree diagram](./assets/discrimination_tree.png)\n\n\n","n":0.066}}},{"i":41,"$":{"0":{"v":"Analogical_Reasoning","n":1},"1":{"v":"\n- Represented by two objects connected by a relationship\n  - features of objects and values of features of objects can create similarity as well\n\n## Cross Domain Analogy\n\npatient/laser problem relation to army/road problem\n\n### Similarity\n\n![similarity spectrum](./assets/analogical_reasoning_similarity_spectrum.png)\n\n- Superficial: deals with features, counts, objects\n- Deep: deals with relationships between objects, relationships between relationships\n\n- Semantic: conceptual similarity between target and source case\n- Pragmatic: similarity of **external** factors, such as goals\n- Structural: similarity between representational structures\n\n## Phases\n\nCase base reasoning is very similar to analogical reasoning, but analogical reasoning has a mapping and transfer step in addition.\n\n### Retrieval\n\nWhen talking about the relationship of an electron to a neutron in an atom, retrive example of planet relationship to sun in the solar system.\n\nType of similarity is important, see ##Similarity\n\ncase-based reasioning would likely only pull superficially similar or deeply and superficially similar analogies.\n\n### Mapping\n\nWhat in the source problem pertains to what in the target problem.\n\nSolve the correspondence problem:\n\n- give higher-order relationships priority over lower-order relationships\n- 18.11 lecture\n- patient/army analogy mapping should be based on goal. Army's goal is to kill king, laser's goal is to kill tumor\n\n### Transfer\n\n![target/source](./assets/analogical_reasoning_target_source.png)\n\nOnce correspondence has been established:\ninduce a pattern of relationships\n\n- goal: kill king / remove tumor\n- obsticle: road mines / healthy tissue\n- resource: soldiers / lasers\n- strategy: decompose resource, arrive at location at the same time\n\n![graph](./assets/analogical_resoning_graph.png)\n\nIn a graphical representation, the vertices represent the objects and features, the edges are representing relationships\n\n### Evaluation\n\nEvaluation can reach out to prior states, such as retrieval, mapping or transfer to help a source get closer to the goal.\n\n### Storage\n\nStore case and solution as a source case and keep for case-based reasoning\n\n## Compound Analogy\n\nUse parts of several analogies to combine into one source.\n\n## Issues w/ Analogical Reasoning\n\n- different cases may not have common vocabulary\n- Abstraction and transformation\n- Compound and compositional analogies\n- Visuospacial analogies\n- Conceptual combination\n","n":0.058}}},{"i":42,"$":{"0":{"v":"Advanced_Topics","n":1},"1":{"v":"\n## Visuospatial Reasoning\n\nKnowledge wherein causality is, at most, implicit\n\nEx. write a story given a picture\n\nCan we just use similar mapping of actions to picture w/o abstracting causal patterns\n\nAI agent creates propositional representation out of visuospatial reasoning -> Archydas\n\n## Analogical Representation\n\n- Analogical reasoning - think semantic nets x to y\n  - amodal representation\n- Analogical representation - think picture to picture w/ afine mutations\n  - modal representation\n\n## Systems Thinking\n\nMultiple levels of abstractions including some invisible layer that needs to be inferred\n\n### Structure-Behavior-Function\n\n- SBF, but not the bitcoin one...\n\n- Level 1: what does a flashlight do\n- Level 2: structure of the flashlight\n- Level 3: behavior - state-transitions-state transitions\n\n","n":0.098}}}]}
